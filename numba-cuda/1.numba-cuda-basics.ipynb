{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Numba for GPUs\n",
    "\n",
    "You may use Numba to generate GPU code from high-level Python functions. Both CUDA and ROCm GPUs (NVIDIA and AMD, respectively) are supported, but we are going to focus only on the CUDA capabilities of Numba.\n",
    "\n",
    "There is an almost one-to-one mapping between the Numba high-level Python abstractions and the different CUDA constructs. This practically means that, as a programmer, you need to take care explicitly of the host/device memory management and you need to be aware of the CUDA programming model.\n",
    "\n",
    "This demo will teach you to write your first CUDA kernels using Numba. It will cover the basic CUDA programming principles, but it should be enough to kick start you in GPU programming. More specifically, we will cover the following topics:\n",
    "\n",
    "- Writing a GPU kernel\n",
    "- Moving data to/from the GPU.\n",
    "- Spawning a GPU kernel\n",
    "- Profiling a GPU kernel\n",
    "- Optimizing memory accesses\n",
    "- Making use of the shared memory\n",
    "\n",
    "<mark>We will not cover CUDA streams.</mark>\n",
    "\n",
    "## Verify that Numba sees the GPU and understands CUDA\n",
    "\n",
    "First thing is to check if Numba can detect the GPU. You can achieve this by running the `numba` executable that comes with Numba's installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System info:\n",
      "--------------------------------------------------------------------------------\n",
      "__Time Stamp__\n",
      "Report started (local time)                   : 2023-06-22 13:23:22.501767\n",
      "UTC start time                                : 2023-06-22 11:23:22.501773\n",
      "Running time (s)                              : 1.061368\n",
      "\n",
      "__Hardware Information__\n",
      "Machine                                       : x86_64\n",
      "CPU Name                                      : haswell\n",
      "CPU Count                                     : 24\n",
      "Number of accessible CPUs                     : 24\n",
      "List of accessible CPUs cores                 : 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      "CFS Restrictions (CPUs worth of runtime)      : None\n",
      "\n",
      "CPU Features                                  : 64bit aes avx avx2 bmi bmi2 cmov\n",
      "                                                crc32 cx16 cx8 f16c fma fsgsbase\n",
      "                                                fxsr invpcid lzcnt mmx movbe\n",
      "                                                pclmul popcnt rdrnd sahf sse sse2\n",
      "                                                sse3 sse4.1 sse4.2 ssse3 xsave\n",
      "                                                xsaveopt\n",
      "\n",
      "Memory Total (MB)                             : 64199\n",
      "Memory Available (MB)                         : 60312\n",
      "\n",
      "__OS Information__\n",
      "Platform Name                                 : Linux-5.3.18-24.46_6.0.29-cray_ari_c-x86_64-with-glibc2.26\n",
      "Platform Release                              : 5.3.18-24.46_6.0.29-cray_ari_c\n",
      "OS Name                                       : Linux\n",
      "OS Version                                    : #1 SMP Mon Mar 14 09:11:41 UTC 2022 (6c38a31)\n",
      "OS Specific Version                           : ?\n",
      "Libc Version                                  : glibc 2.26\n",
      "\n",
      "__Python Information__\n",
      "Python Compiler                               : GCC 8.1.0 20180502 (Cray Inc.)\n",
      "Python Implementation                         : CPython\n",
      "Python Version                                : 3.9.4\n",
      "Python Locale                                 : en_US.UTF-8\n",
      "\n",
      "__Numba Toolchain Versions__\n",
      "Numba Version                                 : 0.57.0\n",
      "llvmlite Version                              : 0.40.1rc1\n",
      "\n",
      "__LLVM Information__\n",
      "LLVM Version                                  : 14.0.6\n",
      "\n",
      "__CUDA Information__\n",
      "CUDA Device Initialized                       : True\n",
      "CUDA Driver Version                           : 11.4\n",
      "CUDA Runtime Version                          : 11.3\n",
      "CUDA NVIDIA Bindings Available                : False\n",
      "CUDA NVIDIA Bindings In Use                   : False\n",
      "CUDA Minor Version Compatibility Available    : False\n",
      "CUDA Minor Version Compatibility Needed       : False\n",
      "CUDA Minor Version Compatibility In Use       : False\n",
      "CUDA Detect Output:\n",
      "Found 1 CUDA devices\n",
      "id 0    b'Tesla P100-PCIE-16GB'                              [SUPPORTED]\n",
      "                      Compute Capability: 6.0\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 2\n",
      "                                    UUID: GPU-368e07fd-53d6-a49b-69e9-0d2f2a6d0f49\n",
      "                                Watchdog: Disabled\n",
      "             FP32/FP64 Performance Ratio: 2\n",
      "Summary:\n",
      "\t1/1 devices are supported\n",
      "\n",
      "CUDA Libraries Test Output:\n",
      "Finding driver from candidates: libcuda.so, libcuda.so.1, /usr/lib/libcuda.so, /usr/lib/libcuda.so.1, /usr/lib64/libcuda.so, /usr/lib64/libcuda.so.1...\n",
      "Using loader <class 'ctypes.CDLL'>\n",
      "\ttrying to load driver...\tok, loaded from libcuda.so\n",
      "Finding nvvm from CUDA_HOME\n",
      "\tnamed  libnvvm.so.4.0.0\n",
      "\ttrying to open library...\tok\n",
      "Finding cudart from CUDA_HOME\n",
      "\tnamed  libcudart.so.11.3.58\n",
      "\ttrying to open library...\tok\n",
      "Finding cudadevrt from CUDA_HOME\n",
      "\tnamed  libcudadevrt.a\n",
      "Finding libdevice from CUDA_HOME\n",
      "\ttrying to open library...\tok\n",
      "\n",
      "\n",
      "__NumPy Information__\n",
      "NumPy Version                                 : 1.24.3\n",
      "NumPy Supported SIMD features                 : ('MMX', 'SSE', 'SSE2', 'SSE3', 'SSSE3', 'SSE41', 'POPCNT', 'SSE42', 'AVX', 'F16C', 'FMA3', 'AVX2')\n",
      "NumPy Supported SIMD dispatch                 : ('SSSE3', 'SSE41', 'POPCNT', 'SSE42', 'AVX', 'F16C', 'FMA3', 'AVX2', 'AVX512F', 'AVX512CD', 'AVX512_KNL', 'AVX512_KNM', 'AVX512_SKX', 'AVX512_CLX', 'AVX512_CNL', 'AVX512_ICL')\n",
      "NumPy Supported SIMD baseline                 : ('SSE', 'SSE2', 'SSE3')\n",
      "NumPy AVX512_SKX support detected             : False\n",
      "\n",
      "__SVML Information__\n",
      "SVML State, config.USING_SVML                 : False\n",
      "SVML Library Loaded                           : False\n",
      "llvmlite Using SVML Patched LLVM              : True\n",
      "SVML Operational                              : False\n",
      "\n",
      "__Threading Layer Information__\n",
      "TBB Threading Layer Available                 : False\n",
      "+--> Disabled due to Unknown import problem.\n",
      "OpenMP Threading Layer Available              : True\n",
      "+-->Vendor: GNU\n",
      "Workqueue Threading Layer Available           : True\n",
      "+-->Workqueue imported successfully.\n",
      "\n",
      "__Numba Environment Variable Information__\n",
      "None found.\n",
      "\n",
      "__Conda Information__\n",
      "Conda not available.\n",
      "\n",
      "__Installed Packages__\n",
      "Package                  Version\n",
      "------------------------ ---------------------\n",
      "asttokens                2.2.1\n",
      "backcall                 0.2.0\n",
      "bokeh                    3.1.1\n",
      "cffi                     1.15.1\n",
      "click                    8.1.3\n",
      "cloudpickle              2.2.1\n",
      "comm                     0.1.3\n",
      "contourpy                1.1.0\n",
      "cupy-cuda113             10.6.0\n",
      "cycler                   0.11.0\n",
      "Cython                   0.29.35\n",
      "dask                     2023.6.0\n",
      "debugpy                  1.6.7\n",
      "decorator                5.1.1\n",
      "distributed              2023.6.0\n",
      "docopt                   0.6.2\n",
      "entrypoints              0.4\n",
      "executing                1.2.0\n",
      "fastrlock                0.8.1\n",
      "fonttools                4.40.0\n",
      "fsspec                   2023.6.0\n",
      "graphviz                 0.20.1\n",
      "h5py                     3.9.0\n",
      "importlib-metadata       6.7.0\n",
      "importlib-resources      5.12.0\n",
      "ipcmagic-cscs            1.1.0\n",
      "ipykernel                6.23.2\n",
      "ipyparallel              8.6.1\n",
      "ipython                  8.14.0\n",
      "jax                      0.4.12\n",
      "jaxlib                   0.4.12+cuda11.cudnn86\n",
      "jedi                     0.18.2\n",
      "Jinja2                   3.1.2\n",
      "jupyter_client           8.2.0\n",
      "jupyter_core             5.3.1\n",
      "kiwisolver               1.4.4\n",
      "llvmlite                 0.40.1rc1\n",
      "locket                   1.0.0\n",
      "lz4                      4.3.2\n",
      "MarkupSafe               2.1.3\n",
      "matplotlib               3.7.1\n",
      "matplotlib-inline        0.1.6\n",
      "ml-dtypes                0.2.0\n",
      "mpi4py                   3.1.4\n",
      "msgpack                  1.0.5\n",
      "nest-asyncio             1.5.6\n",
      "numba                    0.57.0\n",
      "numpy                    1.24.3\n",
      "nvidia-cublas-cu11       11.11.3.6\n",
      "nvidia-cuda-cupti-cu11   11.8.87\n",
      "nvidia-cuda-nvcc-cu11    11.8.89\n",
      "nvidia-cuda-runtime-cu11 11.8.89\n",
      "nvidia-cudnn-cu11        8.9.2.26\n",
      "nvidia-cufft-cu11        10.9.0.58\n",
      "nvidia-cusolver-cu11     11.4.1.48\n",
      "nvidia-cusparse-cu11     11.7.5.86\n",
      "opt-einsum               3.3.0\n",
      "packaging                23.1\n",
      "pandas                   2.0.2\n",
      "parso                    0.8.3\n",
      "partd                    1.4.0\n",
      "pexpect                  4.8.0\n",
      "pickleshare              0.7.5\n",
      "Pillow                   9.5.0\n",
      "pip                      23.1.2\n",
      "platformdirs             3.7.0\n",
      "prompt-toolkit           3.0.38\n",
      "psutil                   5.9.5\n",
      "ptyprocess               0.7.0\n",
      "pure-eval                0.2.2\n",
      "pyarrow                  12.0.1\n",
      "pycparser                2.21\n",
      "Pygments                 2.15.1\n",
      "pyparsing                3.1.0\n",
      "python-dateutil          2.8.2\n",
      "pytz                     2023.3\n",
      "PyYAML                   6.0\n",
      "pyzmq                    25.1.0\n",
      "scipy                    1.10.1\n",
      "setuptools               49.2.1\n",
      "six                      1.16.0\n",
      "sortedcontainers         2.4.0\n",
      "stack-data               0.6.2\n",
      "tblib                    1.7.0\n",
      "toolz                    0.12.0\n",
      "tornado                  6.3.2\n",
      "tqdm                     4.65.0\n",
      "traitlets                5.9.0\n",
      "typing_extensions        4.6.3\n",
      "tzdata                   2023.3\n",
      "urllib3                  2.0.3\n",
      "wcwidth                  0.2.6\n",
      "xyzservices              2023.5.0\n",
      "zict                     3.0.0\n",
      "zipp                     3.15.0\n",
      "\n",
      "No errors reported.\n",
      "\n",
      "\n",
      "__Warning log__\n",
      "Warning: Conda not available.\n",
      " Error was [Errno 2] No such file or directory: 'conda'\n",
      "\n",
      "Warning (no file): /sys/fs/cgroup/cpuacct/cpu.cfs_quota_us\n",
      "Warning (no file): /sys/fs/cgroup/cpuacct/cpu.cfs_period_us\n",
      "--------------------------------------------------------------------------------\n",
      "If requested, please copy and paste the information between\n",
      "the dashed (----) lines, or from a given specific section as\n",
      "appropriate.\n",
      "\n",
      "=============================================================\n",
      "IMPORTANT: Please ensure that you are happy with sharing the\n",
      "contents of the information present, any information that you\n",
      "wish to keep private you should remove before sharing.\n",
      "=============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!numba -s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this command does not work out of the box for your Numba installation, you may have to set the `CUDA_HOME` environment variable to point to your CUDA installation.\n",
    "\n",
    "## Writing the first kernel\n",
    "\n",
    "Here is a vector addition kernel that takes two vectors as input and writes the sum in a third vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba.cuda as cuda\n",
    "\n",
    "\n",
    "@cuda.jit     ## specialized \n",
    "def _vecadd_cuda(z, x, y):\n",
    "    i = cuda.grid(1)\n",
    "    N = x.shape[0]\n",
    "    if i >= N:\n",
    "        return\n",
    "\n",
    "    z[i] = x[i] + y[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**the `kernels` have no `return` value, they are executed BY the CPU ON the GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Pretty simple, right? Let's explain each line of this code.\n",
    "\n",
    "The `@cuda.jit` decorator will compile the following function into a CUDA kernel at runtime. We will see the cost of it later on.\n",
    "\n",
    "A CUDA kernel in Numba is a Python function that does *not* return a value. This is in accordance with CUDA, where kernel functions are declared `void`. The arguments of the function can be either Numpy arrays or scalars of a Numba recognized type.\n",
    "\n",
    "CUDA kernels specify the work to be done by a single GPU thread. Due to the massive hardware parallelism available on the GPU, a single GPU thread gets only a tiny portion of work, in this case, the sumation of a single element in the vectors. Since each element is independent from each other, we can safely create as many threads as the elements of the target vectors and spawn them (we will see how later).\n",
    "\n",
    "Threads on the GPU as organized in groups, called CUDA blocks. There are constraints on the maximum number of threads a block can contain. For the P100 GPUs on Daint, this is 1024. This means that you will need multiple blocks to calculate the sum of large vectors. The blocks that comprise a kernel form the *grid*. Threads inside a block are numbered sequentially and the blocks inside the grid are numbered, too. The figure below shows the arrangement of threads for the vector addition example.\n",
    "\n",
    "![Arrangement of threads in CUDA blocks](figs/cuda-blocks.png)\n",
    "\n",
    "In order to obtain the i-th element of the vector given a block size $B$, you would have to calculate the following:\n",
    "\n",
    "\\begin{equation}\n",
    "i = i_{b}B + i_{t}\n",
    "\\end{equation}\n",
    "\n",
    "where $i_{b}$ is the block index and $i_{t}$ is the thread index. CUDA provides this information and Numba makes it available, so that the above statement would be written as follows:\n",
    "\n",
    "```python\n",
    "i = cuda.blockIdx.x*cuda.blockDim.x + threadIdx.x\n",
    "```\n",
    "\n",
    "Blocks and grids can be three dimensional, thus the `.x` attribute in all of these variables (the other dimensions are accessed through the `.y` and `.z` attributes).\n",
    "\n",
    "Since obtaining the absolute position of a thread in a CUDA is quite common operation, Numba offers the convenience function `grid()`, which essentially does automatically the above calculation:\n",
    "\n",
    "```python\n",
    "i = cuda.grid(1)\n",
    "```\n",
    "\n",
    "<mark>The argument passed to the `grid()` function is the number of dimensions of the grid.</mark>\n",
    "\n",
    "The next three statements in the code simply check that we don't overrun the arrays in case that their dimension is not a multiple of the block size. In this case, some threads of the last block will remain idle:\n",
    "\n",
    "```python\n",
    "    N = x.shape[0]\n",
    "    if i >= N:\n",
    "        return\n",
    "```\n",
    "\n",
    "> The threads inside a block are run in batches of 32 at once, called *warps*. All the threads of the warp execute the same instruction. If the program control flow diverges for some of the threads of the warp, e.g., due to an `if` condition, the branches will be executed sequentially by disabling the non participating threads. This condition is called *warp divergence* and may incur a performance penalty.\n",
    "\n",
    "Finally, the `z[i] = x[i] + y[i]` computes the actual sum.\n",
    "\n",
    "\n",
    "## Preparing the data for the GPU\n",
    "\n",
    "All Numba CUDA kernels operate on data residing on the GPU. That means that the `x`, `y` and `z` arrays must be  transferred from the host (CPU) to the device (accelerator) before calling the CUDA kernel.\n",
    "\n",
    "Let's create first two vectors on the host:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "N = 1024\n",
    "x = rng.random(N)\n",
    "y = rng.random(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compute our reference result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_ref = x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to transfer the vectors to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `d_x` and `d_y` are Numpy *array-like* objects that have their data mapped in the GPU memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<numba.cuda.cudadrv.devicearray.DeviceNDArray at 0x155548c50e80>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `z` array, we don't need to create it on the host and copy it, since it is essentially an output only array. We can simply allocate it directly on the GPU and copy it out when the kernel finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_z = cuda.device_array_like(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will simply allocate an array like `x`, i.e., with the same element type, shape and order, on the GPU.\n",
    "\n",
    "Data is ready, now it's time to call the kernel!\n",
    "\n",
    "## Invoking a CUDA kernel\n",
    "\n",
    "When invoking a CUDA kernel you have to specify the block size to use and the corresponding grid size. Picking the right size of block is not always straightforward, but usually values between 64 and 256 are good enough. \n",
    "\n",
    "> The block size has a direct effect on the *occupancy* of each GPU SM, i.e., how much the actual hardware threads of the SM are utilized, and as a result to the occupancy of the whole GPU. Performance-wise, though, it does not have such a big impact. Nvidia provides a nice tool for calculating the occupancy of the GPU, that you can find [here](https://docs.nvidia.com/cuda/cuda-occupancy-calculator/CUDA_Occupancy_Calculator.xls).\n",
    "\n",
    "For our kernel, we will select a block size of 128 threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having decided the block size we need to set up the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_blocks = N // block_size\n",
    "if N % block_size:\n",
    "    num_blocks += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to call the kernel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/daint/UES/6.0.UP04/sandboxes/sarafael/hpcpy2023/lib/python3.9/site-packages/numba/cuda/dispatcher.py:539: NumbaPerformanceWarning: Grid size 8 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "_vecadd_cuda[num_blocks, block_size](d_z, d_x, d_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the block and the grid could have been two-dimensional (not in this example), in which case you would just define them as tuples.\n",
    "\n",
    "## Copying back the results\n",
    "\n",
    "As mentioned before, kernels operate on GPU data only. We need a way to transfer back to the host the result, which is the `d_z` array. Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = d_z.copy_to_host()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's validate the result though to make sure that everything has worked fine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert np.allclose(z_ref, res)\n",
    "[a for a in res == z_ref if a == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "For completeness and easy reference, here is the whole vector addition example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector length = 10000\n",
      "CUDA\n",
      "CPU times: user 49.2 ms, sys: 1.03 ms, total: 50.2 ms\n",
      "Wall time: 58.8 ms\n",
      "NUMPY\n",
      "CPU times: user 20 µs, sys: 10 µs, total: 30 µs\n",
      "Wall time: 33.1 µs\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/daint/UES/6.0.UP04/sandboxes/sarafael/hpcpy2023/lib/python3.9/site-packages/numba/cuda/dispatcher.py:539: NumbaPerformanceWarning: Grid size 79 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: '_Helper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary -: '_Helper'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector length = 1000000\n",
      "CUDA\n",
      "CPU times: user 121 µs, sys: 60 µs, total: 181 µs\n",
      "Wall time: 186 µs\n",
      "NUMPY\n",
      "CPU times: user 0 ns, sys: 3.39 ms, total: 3.39 ms\n",
      "Wall time: 3.39 ms\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: '_Helper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary -: '_Helper'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector length = 100000000\n",
      "CUDA\n",
      "CPU times: user 290 µs, sys: 136 µs, total: 426 µs\n",
      "Wall time: 375 µs\n",
      "NUMPY\n",
      "CPU times: user 156 ms, sys: 236 ms, total: 392 ms\n",
      "Wall time: 392 ms\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: '_Helper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary -: '_Helper'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector length = 100000000\n",
      "CUDA\n",
      "CPU times: user 263 µs, sys: 143 µs, total: 406 µs\n",
      "Wall time: 357 µs\n",
      "NUMPY\n",
      "CPU times: user 181 ms, sys: 213 ms, total: 394 ms\n",
      "Wall time: 393 ms\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: '_Helper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary -: '_Helper'"
     ]
    }
   ],
   "source": [
    "import numba.cuda as cuda\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def _vecadd_cuda(z, x, y):\n",
    "    '''The CUDA kernel'''\n",
    "    i = cuda.grid(1)\n",
    "    N = x.shape[0]\n",
    "    if i >= N:\n",
    "        return\n",
    "\n",
    "    z[i] = x[i] + y[i]\n",
    "\n",
    "\n",
    "# Set up the host vectors\n",
    "Ns = [10000, 1000000, 100000000, 100000000]\n",
    "for N in Ns:\n",
    "    #N = 500000000\n",
    "    print(f\"Vector length = {N}\")\n",
    "    x = rng.random(N)\n",
    "    y = rng.random(N)\n",
    "\n",
    "    # Copy and allocate data on the device\n",
    "    d_x = cuda.to_device(x)\n",
    "    d_y = cuda.to_device(y)\n",
    "    d_z = cuda.device_array_like(x)\n",
    "\n",
    "    # Set up the kernel invocation\n",
    "    block_size = 128\n",
    "    num_blocks = N // block_size\n",
    "    if N % block_size:\n",
    "        num_blocks += 1\n",
    "\n",
    "    # Call the kernel\n",
    "    print(\"CUDA\")\n",
    "    %time _vecadd_cuda[num_blocks, block_size](d_z, d_x, d_y)\n",
    "    \n",
    "    print(\"NUMPY\")\n",
    "    %time x + y\n",
    "\n",
    "    # Copy back the result to the host\n",
    "    res = d_z.copy_to_host()\n",
    "\n",
    "    # Validate the result\n",
    "    assert np.allclose(x + y, res)\n",
    "    print('-'*100)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "> 1. Make the array sufficiently large and time the Numpy version of the sum `x + y`.\n",
    "> 2. Now time the call to the CUDA kernel with `%timeit -n1 -r1`. What do you see?\n",
    "> 3. Try timing the CUDA kernels with `%timeit -n1 -r2`. What is happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpcpy2023",
   "language": "python",
   "name": "hpcpy2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
